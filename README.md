# Benchmarking Uncertainty Estimation of Deep Learning Models Using Synthetic Dataset

In this project we create a platform for the deep learning researchers to test the DNN models for uncertainty using **ground truth uncertainty labels** and **synthetic datasets**.

## Table of Contents

## About

## Abstract

## Introduction
* With the increase in the deployment of neural networks in different safety-critical domains like medicine, aviation, autonomous robots, and self-driving cars, the need for understanding and explaining what a neural network doesn't know became crucial.
* To address this need research has provided a concept called uncertainty estimation in deep learning and the state-of-the-art uncertainty estimation techniques include
    * Dropout
    * Ensebles
    * Bayesian Networks
    * Spectral-normalized Neural Gaussian Process (SNGP)
    * Deterministic uncertainty quantification (DUQ)
* In neural networks a classifier model is forced to decide between all the possible outcomes even though it does not have any clue. 
* In deep learning, uncertainty can be defined as partial or complete lack of knowledge of the outcome of the predictions made by the deep learning models.
* Humans by default have the knowledge of uncertainty from their past experiences and they can distinguish between the uncertain conditions and normal conditions in different environmental scenarios like lighting, occlusion, blur etc.
